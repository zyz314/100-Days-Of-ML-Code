{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn4DA9Hz5PBW+q7LpgpKZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zyz314/100-Days-Of-ML-Code/blob/master/Project%207-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP2oAxCpD9Pk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearner:\n",
        "    def __init__(self, num_states, num_actions, alpha, gamma, rar, radr, dyna, verbose):\n",
        "        \"\"\"\n",
        "        初始化 QLearner 类\n",
        "\n",
        "        参数:\n",
        "        num_states (int): 状态数\n",
        "        num_actions (int): 动作数\n",
        "        alpha (float): 学习率\n",
        "        gamma (float): 折扣率\n",
        "        rar (float): 随机动作率\n",
        "        radr (float): 随机动作衰减率\n",
        "        dyna (int): Dyna更新次数\n",
        "        verbose (bool): 是否打印调试信息\n",
        "        \"\"\"\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.rar = rar\n",
        "        self.radr = radr\n",
        "        self.dyna = dyna\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # 创建一个大小为(num_states, num_actions)的Q表，并初始化为0\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "        # 如果使用Dyna-Q，初始化模型存储\n",
        "        self.experience = []\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        根据当前状态选择一个行动，考虑探索和利用的平衡\n",
        "\n",
        "        参数:\n",
        "        state (int): 当前状态\n",
        "\n",
        "        返回:\n",
        "        int: 选择的行动\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.rar:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def querysetstate(self, state):\n",
        "        \"\"\"\n",
        "        设置当前状态为state，并选择一个行动\n",
        "\n",
        "        参数:\n",
        "        state (int): 当前状态\n",
        "\n",
        "        返回:\n",
        "        int: 选择的行动\n",
        "        \"\"\"\n",
        "        self.state = state\n",
        "        self.action = self.choose_action(state)\n",
        "        return self.action\n",
        "\n",
        "    def query(self, new_state, reward):\n",
        "        \"\"\"\n",
        "        使用新的状态new_state和奖励reward更新Q表\n",
        "\n",
        "        参数:\n",
        "        new_state (int): 新状态\n",
        "        reward (float): 奖励\n",
        "\n",
        "        返回:\n",
        "        int: 选择的下一个行动\n",
        "        \"\"\"\n",
        "        # 获取当前Q值\n",
        "        current_q_value = self.q_table[self.state, self.action]\n",
        "        # 计算最大Q值\n",
        "        next_max_q_value = np.max(self.q_table[new_state])\n",
        "        # 更新Q值\n",
        "        self.q_table[self.state, self.action] = (\n",
        "            (1 - self.alpha) * current_q_value + self.alpha * (reward + self.gamma * next_max_q_value)\n",
        "        )\n",
        "\n",
        "        # 存储经验\n",
        "        if self.dyna > 0:\n",
        "            self.experience.append((self.state, self.action, new_state, reward))\n",
        "            for _ in range(self.dyna):\n",
        "                state, action, state_next, reward_dyn = self.experience[np.random.randint(len(self.experience))]\n",
        "                max_q_value_dyn = np.max(self.q_table[state_next])\n",
        "                self.q_table[state, action] = (\n",
        "                    (1 - self.alpha) * self.q_table[state, action]\n",
        "                    + self.alpha * (reward_dyn + self.gamma * max_q_value_dyn)\n",
        "                )\n",
        "\n",
        "        # 选择下一个行动\n",
        "        self.action = self.choose_action(new_state)\n",
        "\n",
        "        # 更新随机动作率\n",
        "        self.rar *= self.radr\n",
        "\n",
        "        # 更新当前状态\n",
        "        self.state = new_state\n",
        "\n",
        "        return self.action\n",
        "\n",
        "    def author(self):\n",
        "        \"\"\"\n",
        "        返回你的Georgia Tech用户名\n",
        "\n",
        "        返回:\n",
        "        str: 用户名\n",
        "        \"\"\"\n",
        "        return 'yzheng438'\n",
        "\n",
        "    def study_group(self):\n",
        "        \"\"\"\n",
        "        返回学习小组成员的Georgia Tech用户名，以逗号分隔\n",
        "\n",
        "        返回:\n",
        "        str: 用户名列表\n",
        "        \"\"\"\n",
        "        return 'yzheng438'\n",
        "\n",
        "# 初始化Q学习器\n",
        "learner = QLearner(num_states=100, num_actions=4, alpha=0.2, gamma=0.9, rar=0.98, radr=0.999, dyna=0, verbose=False)\n",
        "\n",
        "# 读取导航问题的地图文件，初始化起始状态\n",
        "def read_map_file(filename):\n",
        "    \"\"\"\n",
        "    读取地图文件，返回地图和起始位置\n",
        "\n",
        "    参数:\n",
        "    filename (str): 地图文件的路径\n",
        "\n",
        "    返回:\n",
        "    tuple: 地图和起始位置\n",
        "    \"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        map_data = [list(map(int, line.strip().split(','))) for line in f]\n",
        "\n",
        "    start_position = next((i, row.index(2)) for i, row in enumerate(map_data) if 2 in row)\n",
        "\n",
        "    return map_data, start_position\n",
        "\n",
        "map_data, start_position = read_map_file('map_file.csv')\n",
        "state = start_position[0] * 10 + start_position[1]\n",
        "\n",
        "# 设置学习器的初始状态\n",
        "action = learner.querysetstate(state)\n",
        "\n",
        "# 定义一个函数来计算奖励\n",
        "def compute_reward(state, map_data):\n",
        "    \"\"\"\n",
        "    计算奖励\n",
        "\n",
        "    参数:\n",
        "    state (tuple): 当前状态 (行, 列)\n",
        "    map_data (list): 地图\n",
        "\n",
        "    返回:\n",
        "    int: 奖励\n",
        "    \"\"\"\n",
        "    row, col = state\n",
        "    if map_data[row][col] == 0:\n",
        "        return -1\n",
        "    if map_data[row][col] == 5:\n",
        "        return -100\n",
        "    if map_data[row][col] == 3:\n",
        "        return +1\n",
        "    return 0\n",
        "\n",
        "# 定义一个函数来更新状态\n",
        "def update_state(state, action):\n",
        "    \"\"\"\n",
        "    更新状态\n",
        "\n",
        "    参数:\n",
        "    state (tuple): 当前状态 (行, 列)\n",
        "    action (int): 行动 (0: 北, 1: 东, 2: 南, 3: 西)\n",
        "\n",
        "    返回:\n",
        "    tuple: 新状态 (行, 列)\n",
        "    \"\"\"\n",
        "    row, col = state\n",
        "    if action == 0:\n",
        "        return row - 1, col\n",
        "    if action == 1:\n",
        "        return row, col + 1\n",
        "    if action == 2:\n",
        "        return row + 1, col\n",
        "    if action == 3:\n",
        "        return row, col - 1\n",
        "\n",
        "# 重复进行学习直到收敛\n",
        "converged = False\n",
        "while not converged:\n",
        "    # 根据当前行动更新状态\n",
        "    row, col = divmod(state, 10)\n",
        "    next_state = update_state((row, col), action)\n",
        "    next_state_flat = next_state[0] * 10 + next_state[1]\n",
        "\n",
        "    # 根据新状态获取奖励\n",
        "    reward = compute_reward(next_state, map_data)\n",
        "\n",
        "    # 使用新的状态和奖励更新Q表并获取下一个行动\n",
        "    action = learner.query(next_state_flat, reward)\n",
        "\n",
        "    # 如果到达目标位置，重新设置起始状态\n",
        "    if map_data[next_state[0]][next_state[1]] == 3:\n",
        "        state = start_position[0] * 10 + start_position[1]\n",
        "    else:\n",
        "        state = next_state_flat\n",
        "\n",
        "    # 判断是否收敛的逻辑 (根据具体需求实现)\n",
        "    # 这里假设我们有一个简单的收敛判断方法\n",
        "    # converged = check_convergence()\n"
      ]
    }
  ]
}